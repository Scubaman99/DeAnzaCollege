{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzMyg0BwXl5u"
   },
   "source": [
    "#  Statistical Hypothesis Testing\n",
    "\n",
    "# Reading\n",
    "\n",
    "[Chapter 11: 11.1 - 11.4](https://inferentialthinking.com/chapters/11/Testing_Hypotheses.html)<br><br>\n",
    "\n",
    "In this notebook we will apply our understanding of random samples and empirical distributions to check or test a _hypothesis_ about a dataset. A hypothesis is an assumption that we make on a dataset's parameter.\n",
    "\n",
    "Examples of hypotheses that we test: the new website increases the chance of customers purchasing a product, or, the new drug lowers the patient's blood pressure better than an existing drug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJkeA9U9ZPIT"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb9C0VPyZQH_"
   },
   "source": [
    "## Assessing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrNGdcluYFRV"
   },
   "source": [
    "Before we can test a hypothesis, we need to learn how to create a _model_ that can be used to test the hypothesis. In data science a _model_ is a set of assumptions about the data, including how the data was obtained. Our work is to determine whether the model is good or not.\n",
    "\n",
    "We will evaluate a model by using the jury selection example discussed in the [textbook](https://inferentialthinking.com/chapters/11/1/Assessing_a_Model.html#jury-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjPCuDXLcSvE"
   },
   "source": [
    "A jury pool is a group of people that are selected to make decisions about a civil or criminal trial. A jury pool is supposed to be racially diverse and representative of the different communities where the trial takes place.\n",
    "\n",
    "The textbook discussed a real-life case in 1962, when a Black man named Robert Swank appealed his conviction on the grounds that Black people were often excluded from the jury pool, which led to an unfair trial for him. For Robert Swank's trial, 8 Black men were selected for the jury pool of 100 men, while Black men made up 26% of the population from which jurors were chosen. In the end, the court denied Robert Swank's appeal because it determined that the selection of jurors was fair.\n",
    "\n",
    "We now determine whether or not the court was correct by:\n",
    "- Making the assumption that the jury selection was a fair model where jurors were randomly selected from the population.\n",
    "- With this model in mind, we simulate the selection of jurors and compare the simulated jury pool with the actual jury pool.\n",
    "- If the results are not similar, then it is evidence that the jury selection was not fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRySjQHLhTd4"
   },
   "source": [
    "The statistic or parameter of the data that we will measure is the number of Black men on the jury panel.\n",
    "\n",
    "First we use the following function to simulate this statistic with a sample size of 100, the same size as the actual jury pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzeA9lZT8o4V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this function accepts a sample size and the proportions or ratio\n",
    "# of each category of the population\n",
    "\n",
    "def sample_proportions(sample_size, proportions):\n",
    "    # draw random sample from a population with the appropriate proportions\n",
    "    categories = np.arange(len(proportions))\n",
    "    sample = np.random.choice(categories, size=sample_size, p=proportions)\n",
    "\n",
    "    # count occurrences of each category\n",
    "    counts = np.bincount(sample, minlength=len(proportions))\n",
    "\n",
    "    # convert counts to proportions\n",
    "    return counts / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwl1BmK__d6Q"
   },
   "source": [
    "We test the function by running it a few times with a sample of 100 and the [0.26, 0.74] ratio for Blacks and non-Blacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWDeJ8Ju_-Py",
    "outputId": "1da1c8b0-3b80-406c-9bd1-e06c3b7cb37f"
   },
   "outputs": [],
   "source": [
    "proportions = sample_proportions(100, [0.26, 0.74])\n",
    "print(\"Proportions for Blacks and Non-Blacks:\", proportions)\n",
    "print(\"Number of Blacks:\", int(100 * proportions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik4N5FxS1jJG"
   },
   "source": [
    "By running the Code cells above a few times, we can see that the number of Blacks selected is around 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OxqDc3xsaEP"
   },
   "source": [
    "Next we simulate the jury selection by running `sample_proportions` 10,000 times and plotting the empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6NgiAQys9AL",
    "outputId": "1619c2ca-e8ea-4d49-8fa2-3a1bbc7b6bee"
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(10000):\n",
    "    proportions = sample_proportions(100, [0.24, 0.76])\n",
    "    L.append(100 * proportions[0])\n",
    "array = np.array(L)\n",
    "print(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "toKDpcmPtUwj",
    "outputId": "06141d2d-9dd5-4d4a-b18c-f028afa013f3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(array, density=True, bins=np.arange(5.5, 46.6), edgecolor='black')\n",
    "plt.title(\"Empirical Distribution of Statistic\")\n",
    "plt.xlabel(\"Count of Black Jurors\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr7I6CFwuT3y"
   },
   "source": [
    "We can see that with Black men being 26% of the population, if the jury selection was a fair random selection, we should end up between 20-30 Black men being selected, not 8 as with Robert Swank's trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIXTSmgdeBd0"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_OvpHBOeCmL"
   },
   "source": [
    "## Models with multiple categories\n",
    "\n",
    "In the previous example we worked with a model with one category: the number of Black men selected for the jury pool. With one category, it's simple to use the category as the statistic that we want to measure. Now we look at a model with multiple categories and see how to determine an appropriate statistic that we can measure.\n",
    "\n",
    "Continuing with the jury selection process, we will look at selected jurors across multiple ethnicities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGb_NGdR6v5z"
   },
   "source": [
    "We follow the [textbook](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html#composition-of-panels-in-alameda-county)'s example of jury selection in Alameda County, California. Between 2009 and 2010, there were 1453 people who reported for jury duty for 11 felony cases. Here are the data from the jury pools for these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9uF2J2QN7zW9",
    "outputId": "e50b194a-611d-4fc4-e7f6-95c0ade7a8c8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jury = pd.DataFrame ({'Ethnicity': ['Asian/PI', 'Black/AA', 'Caucasian', 'Hispanic', 'Other'],\n",
    "                      'Eligible':[0.15, 0.18, 0.54, 0.12, 0.01],\n",
    "                      'Selected':[0.26, 0.08, 0.54, 0.08, 0.04]})\n",
    "jury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2njc1bLP85Ye"
   },
   "source": [
    "The data in the `Eligible` column are the proportions of population who are eligible to be jurors. The data in the `Selected` column are proportions of people who are actually selected for the jury pool.\n",
    "\n",
    "Using data visualization, we plot the `Eligible` and the `Selected` data of each ethnicity with a bar chart.\n",
    "\n",
    "In the plot we put the bars for `Eligible` and `Selected` columns next to each other for easy visual comparison. To do this we need to adjust the placements of the bars and the yticks as shown in the comments in the Code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "MH-0WTem-Iwn",
    "outputId": "b00882b1-b008-4efc-983f-366ba2dd9f24"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# You don't need to write the code below.\n",
    "# The code shows a visual comparison between the Eligible and Selected columns\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "bar_width = 0.25\n",
    "# get the index of each row\n",
    "index = jury.index.values\n",
    "\n",
    "# plot the row index vs the data for 'Eligible'\n",
    "plt.barh(index, jury['Eligible'], bar_width, label='Eligible')\n",
    "\n",
    "# plot the row index vs the data for 'Selected'\n",
    "# and adjust the location of the bar by bar_width\n",
    "plt.barh(index + bar_width, jury['Selected'], bar_width, label='Selected')\n",
    "\n",
    "# convert the index numbers into the corresponding data in 'Ethnicity'\n",
    "plt.yticks(index + bar_width / 2, jury['Ethnicity'])\n",
    "\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Ethnicity')\n",
    "plt.title('Comparison of Eligible and Selected Jurors')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmA0wXSiBQBk"
   },
   "source": [
    "We can see that fewer Black/AA and Hispanic people were selected compared to the eligible people in these two ethnic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmDt8zkmF13P"
   },
   "source": [
    "We now want to simulate the random selection of 1453 people, which would be a fair selection, and compare the result with the actual selection above.\n",
    "\n",
    "First we need to determine an appropriate statistic that incorporates the multiple ethnicities in the jury selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUYrQDITB3M0"
   },
   "source": [
    "- Step 1:<br>\n",
    "We calculate the difference between the `Eligible` and `Selected` columns by subtracting the data between the 2 columns and find their absolute value. Then we store both the difference and the absolute value as 2 new columns of `jury`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-X05pvfOCO3-",
    "outputId": "6b1b9c6f-0330-4fb0-dfaa-e7488517904c"
   },
   "outputs": [],
   "source": [
    "diff = jury['Eligible'] - jury['Selected']\n",
    "jury['Difference'] = round(diff, 2)\n",
    "jury['Abs Diff'] = np.abs(diff)\n",
    "jury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeQiMK1xyxXB",
    "outputId": "7a486b1e-f19c-4df6-df05-62b9357e8a82"
   },
   "outputs": [],
   "source": [
    "print(\"Sum of Eligible column:\", round(jury.Eligible.sum(),2))\n",
    "print(\"Sum of Selected column:\", round(jury.Selected.sum(),2))\n",
    "print(\"Sum of Difference column:\", round(jury.Difference.sum(),2))\n",
    "print(\"Sum of Abs Diff column:\", round(jury['Abs Diff'].sum(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8fyE_MoClW3"
   },
   "source": [
    "- Step 2:<br>\n",
    "Observing the columns of `jury` above, we note that:<br>\n",
    "> - The proportions in `Eligible` add up to 1 or 100% of the population.\n",
    ">- Likewise, the proportions in `Selected` add up to 1 or 100% of the population.\n",
    ">- The values in `Difference` add up to 0 since the positive differences add up to 0.14 and the negative differences add up to -0.14. This makes sense because if there are more jurors from one ethnicity, it means there has to be fewer jurors from a different ethnicity.\n",
    ">- The values in `Abs Diff` add up to 0.28, which is twice of 0.14. The first 0.14 is the sum of the positive differences and the second 0.14 is the sum of the negative differences, after we remove the negative sign due to the absolute value.\n",
    "\n",
    "The 0.14 is a significant value, it is called the _total variation distance (TVD)_. The TVD is a measurement of how close the distributions of the eligible jurors and the selected jurors are. The smaller the TVD value, the closer the two distributions are.\n",
    "\n",
    "We will use the TVD as the statistic for our simulation of the jury selection process.\n",
    "\n",
    "Observing how the sum of the `Abs Diff` column is twice the TVD, we can write a function for the TVD between 2 populations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8pHCSrhCr74",
    "outputId": "91f19384-5ec1-4091-9f2b-4f1622bea162"
   },
   "outputs": [],
   "source": [
    "def get_TVD(distribution1, distribution2):\n",
    "    return np.sum(np.abs(distribution1 - distribution2)) / 2\n",
    "\n",
    "# test the function with the data from jury:\n",
    "print(get_TVD(jury['Eligible'], jury['Selected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTkPSQ-14pOM"
   },
   "source": [
    "- Step 3:<br>\n",
    "We now use the same `sample_proportions` function from earlier to create a sample of 1453 jurors with the proportions of ethnicity from the `Eligible` column. Then we apply `get_TVD` to the sample to get the simulated TVD value and compare it with 0.14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BELda1TGx4h",
    "outputId": "f1961a7e-ab05-427e-9349-ad4cf7412e21"
   },
   "outputs": [],
   "source": [
    "proportions = sample_proportions(1453, jury['Eligible'])\n",
    "print(round(get_TVD(proportions, jury['Eligible']),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6dZ48WjLC3b"
   },
   "source": [
    "The sample TVD value is much smaller than 0.14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XZIM4SXHrcf"
   },
   "source": [
    "- Step 4:<br>\n",
    "Now that we can create a sample of jurors and find the TVD, we can simulate the jury selection process by repeating the sampling 5000 times and recording the TVD values. Then we plot the resulting TVD values to get our empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zs0my_bYIZ91",
    "outputId": "b64a0a95-f5dc-4618-fd26-b11c61436624"
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(5000):\n",
    "    proportions = sample_proportions(1453, jury['Eligible'])\n",
    "    L.append(get_TVD(proportions, jury['Eligible']))\n",
    "array = np.array(L)\n",
    "print(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "EB4DZ_GtJKh4",
    "outputId": "11d8c844-4093-4a0c-c495-bef905244cf2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(array, density=True, bins=np.arange(0, 0.2, 0.005), edgecolor='black')\n",
    "plt.title(\"Empirical Distribution of Statistic\")\n",
    "plt.xlabel(\"TVD\")\n",
    "plt.scatter(0.14, 0, color='red', s=30) # plot the actual 0.14 TVD\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXZpX-eXJCbh"
   },
   "source": [
    "The plot shows that a fair jury selection will have a TVD closer to 0, between 0 and 0.05, compared to the 0.14 TVD value of the actual jury selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lfFmGmkNDIb"
   },
   "source": [
    "Our analysis of the jury selection for both Robert Swank's case and the Alameda County cases shows that the jury selection process certainly is not fair and should be improved. There are historical, social, and economic reasons for the bias in jury selection.The [textbook](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html#reasons-for-the-bias) covers these reasons as well as the impact of an unfair jury selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc5lQOtPMAmF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdHiU0ZqMWxy"
   },
   "source": [
    "## Testing Hypotheses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKTLaAN0MCdM"
   },
   "source": [
    "In the previous sections we learn how to create a model about the data, and then run simulation to test whether the model is correct. In this next section we will use the textbook's model of Gregor Mendel's genetic experiments with pea plants to learn concepts and terminology in testing hypotheses.  \n",
    "\n",
    "Gregor Mendel observed that for the pea plant variety that he worked with, about 75% of the plants have purple flowers and 25% of the plants have white flowers. To test whether his observation was valid, Mendel grew 929 pea plants, among which 705 (or 76%) had purple flowers.\n",
    "\n",
    "The model from Mendel's experiment is:  any pea plant has a 75% chance of having purple flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBTb9OkmSjYq"
   },
   "source": [
    "- <u>The Hypotheses</u>\n",
    "\n",
    "Statistical hypothesis testing is a way to make decisions or inferences from data. Hypothesis testing include 2 hypotheses:\n",
    "1. The _null hypothesis_ says that there is no significant difference between any simulated results and expected results. Any difference is due to random chance rather than a real phenomenon.<br>\n",
    "The term _null_ means that any differences in the simulated result from the model's expected result is due to _nothing_ but chance.\n",
    "2. The _alternative hypothesis_ says that any difference in the simulated result from the model's expected result is due to _some reason other than chance_.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F11ATfkrej0K"
   },
   "source": [
    "Considering that Mendel's model is: a pea plant has a 75% chance of having purple flowers:\n",
    "\n",
    "> The null hypothesis is: a pea plant has a 75% chance of having purple flowers when we simulate the model with randomly generated data.\n",
    "\n",
    "> The alternative hypothesis is: the model is not correct. The simulated result will not show a 75% chance of having purple flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1GdOA2wf8xR"
   },
   "source": [
    "- <u>The Test Statistic</u>\n",
    "\n",
    "Since this is a model with 2 categories: purple and white flowers, the statistic will be the TVD.\n",
    "\n",
    "We recall from the Alameda jury selection example above, the TVD of a model with 2 categories is the distance between the two proportions in one category. Since the proportions are [0.75, 0.25], we find the distance as the absolute value of the difference between the simulated purple flower percentage and 75, which is the expected purple flower percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3NDqMHol-dX"
   },
   "outputs": [],
   "source": [
    "def find_TVD(percent_purple):\n",
    "  return np.abs(percent_purple - 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21Yh1Qp5qtI6"
   },
   "source": [
    "Testing the `find_TVD` function by running it with the actual result from Mendel's experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5kI8QRjq3-i",
    "outputId": "31a31cb7-cf49-49de-c75e-7b435276e131"
   },
   "outputs": [],
   "source": [
    "actual_percent_purple = 705 / 929 * 100   # = 76% as Mendel found\n",
    "print(round(find_TVD(actual_percent_purple), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2fEfDIrPdo_"
   },
   "source": [
    "As expected, the TVD is small, less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlrnVeXftAja"
   },
   "source": [
    "- <u>The Simulation and Empirical Distribution</u>\n",
    "\n",
    "As with the jury selection models earlier, we now create one sample of 929 values with the proportions [0.75, 0.25] and find its TVD. Then we simulate with 10,000 samples and plot the resulting TVDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_trPW1XtNtqM",
    "outputId": "ee94ae6c-6acc-4462-91a6-39c3d02f5af2"
   },
   "outputs": [],
   "source": [
    "# create one sample and observe the TVD\n",
    "def create_one_sample():\n",
    "  proportions = sample_proportions(929, [0.75, 0.25])\n",
    "  number_of_purple = int(proportions[0] * 929)\n",
    "  percent_purple = number_of_purple / 929 * 100\n",
    "  return round(find_TVD(percent_purple),2)\n",
    "\n",
    "print(\"TVD for one sample:\", create_one_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "7r0-073LOQWp",
    "outputId": "ef1ac7cb-9ab2-4b04-b4b8-35cedc35c1c9"
   },
   "outputs": [],
   "source": [
    "# simulate with 10,000 samples\n",
    "L = []\n",
    "for i in range(10000):\n",
    "    L.append(create_one_sample())\n",
    "TVD_array = np.array(L)\n",
    "\n",
    "# plot the TVD distribution\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(TVD_array, density=True, edgecolor='black')\n",
    "plt.title(\"Empirical Distribution of Statistic\")\n",
    "plt.xlabel(\"TVD\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbBF1R5qtScc"
   },
   "source": [
    "Mendel's experiment has an actual TVD of 0.89 as we found in a previous step. This value agrees with the distribution: the majority of the samples have a TVD between 0 and 1. This means Mendel's model is good, and the data doesn't reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8exiS9bkLI2"
   },
   "source": [
    "## The p-Value\n",
    "\n",
    "For the hypothesis testing example with Mendel's model above, what if another experiment has a TVD value of 3.5? Looking up where 3.5 is in the histogram above, we see that it is at the right tail end of the distribution, where fewer samples are.\n",
    "\n",
    "Using the `TVD_array` or the list of all TVD values from the 10,000 samples, we can calculate the chance that an experiment will produce an observed statistic of 3.5 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26Lj8KMNtAFd",
    "outputId": "effeb790-0343-4ec3-9c96-3dd5a14b99ac"
   },
   "outputs": [],
   "source": [
    "samples_above_3 = np.count_nonzero(TVD_array >= 3.5)\n",
    "percent_above_3 = samples_above_3 / 10000 * 100\n",
    "percent_above_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cEC6ljTq6Dl"
   },
   "source": [
    "There's only about 1% chance that the samples have TVD values at or above 3.5. Something unusual happened in that experiment because a TVD of 3.5 is unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33ehHw9etdXL"
   },
   "source": [
    "The low chance of getting a TVD value at or above 3.5 brings us to the _p-value_.\n",
    "\n",
    "The p-value of a model is the chance of getting the observed data in the simulated results.\n",
    "- A large p-value (at or greater than 5%) means the observed data is in the expected range of the simulated results. This p-value doesn't reject the null hypothesis.\n",
    "- A small p-value (less than 5%) means the observed data is at the tail end of the simulated results. This range of p-values is called _statistically significant_ and rejects the null hypothesis or supports the alternative hypothesis.\n",
    "- A very small p-value (less that 1%) is called _highly statistically significant_ and rejects the null hypothesis.\n",
    "\n",
    "Note that the 5% cut-off that we use to decide to reject or not reject the null hypothesis is a standard and not an absolute percentage. Depending on the data and hypotheses, the cut-off may be a lower or higher percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U01YXOXvzkoh"
   },
   "source": [
    "In Mendel's experiment, the TVD was 0.89. We calculate the p-value for this observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4jk6jif0CH-",
    "outputId": "1707c1c0-5c12-4d5a-a8f6-ac07c261dbf0"
   },
   "outputs": [],
   "source": [
    "samples_above = np.count_nonzero(TVD_array >= 0.89)\n",
    "percent_above = samples_above / 10000 * 100\n",
    "percent_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoQ5TOlA0OT9"
   },
   "source": [
    "As expected, this is a large p-value and supports the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9j3fXgr085y"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI-bCaCr0-he"
   },
   "source": [
    "## Error Probability\n",
    "\n",
    "Suppose we create a model to test that in a fair coin toss, the chance that the coin lands heads first is 50%.\n",
    "\n",
    "In the Code cell below are the 3 steps to test the model:\n",
    "1. We create a sample of 2000 coin tosess with proportions [0.5, 0.5].\n",
    "2. The statistic is the difference between the number of heads in the simulation and the expected number of heads, which is 1000.\n",
    "3. We simulate with 10,000 samples to plot the distribution of the statistic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "NAZdZFE4F8h6",
    "outputId": "602fa915-7b6b-4f58-b622-122aca2d9286"
   },
   "outputs": [],
   "source": [
    "# steps 1 and 2\n",
    "def create_one_sample():\n",
    "  proportions = sample_proportions(2000, [0.5, 0.5])\n",
    "  number_of_heads = int(proportions[0] * 2000)\n",
    "  return np.abs(number_of_heads - 1000)\n",
    "\n",
    "# step 3:\n",
    "L = []\n",
    "for i in range(10000):\n",
    "    L.append(create_one_sample())\n",
    "statistic_array = np.array(L)\n",
    "\n",
    "# plot the TVD distribution\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(statistic_array, density=True, edgecolor='black')\n",
    "plt.title(\"Distribution of Heads in 2000 Coin Tosses\")\n",
    "plt.xlabel(\"|number of heads - 1000|\")\n",
    "plt.axvline(x=45, color='red', linestyle='--')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU_iEJF2LjSH"
   },
   "source": [
    "The histogram shows that the data supports the null hypothesis: for the majority of samples, the number of heads is close to 1000 and the difference `|number of heads - 1000|` is close to 0.\n",
    "\n",
    "The p-value cut off of 5% is when the difference is around 45 and is denoted by the dashed red line in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geIqnmzRHybw",
    "outputId": "bac3c62d-bde6-4f46-ab63-2033533c59e0"
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(statistic_array >= 45) / 10000 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoShN9LA9xg8"
   },
   "source": [
    "Using our 5% p-value cut off, this means if a coin is tossed 2000 times and the number of heads is above 1045 or below 955, we will conclude that the coin is not a fair coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFB2TfiwOjEt"
   },
   "source": [
    "But conversely, suppose we have a coin that we know is fair, then the histogram above tells us that if we toss the coin 2000 times, there's a 5% chance that the number of heads is above 1045 or below 955. This would cause us to decide _incorrectly_ that the coin is not fair.\n",
    "\n",
    "This leads us to the fact that: If we use a\n",
    "_p_% cutoff for the p-value, and the null hypothesis is not rejected, then there is about a\n",
    "_p_% chance that we will conclude _incorrectly_ that the alternative hypothesis is correct.\n",
    "\n",
    "The p-value cut off is the _error probability_, or the probability that we could be in error with our conclusion.\n",
    "\n",
    "Being aware of the error probability is important. For example, it means that any conclusion made on a breakthrough scientific research should not rely on a single experiment. The experiment needs to be replicated, in case the error probability leads to the wrong conclusion the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TACH57_yUqRu"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4j15cjGUrzj"
   },
   "source": [
    "In this notebook we applied sampling and simulation in models that are used to test hypotheses. The simulated data in our model can reject or fail to reject the null hypothesis, where the hypothesis is our assumption about a parameter of the data. In making conclusion about the hypothesis, we also keep in mind the error probability caused by working with randomly generated data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
