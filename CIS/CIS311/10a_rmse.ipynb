{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIPKfhlx4AsK"
   },
   "source": [
    "# RMSE\n",
    "\n",
    "## Reading\n",
    "\n",
    "[Chapter 15: 15.4 - 15.6](https://inferentialthinking.com/chapters/15/Prediction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnaK6HTiMZ1j"
   },
   "source": [
    "In the previous notebook we developed formulas for the slope and intercept of the regression line for data that cluster uniformly around the regression line. In this notebook we'll see that what we learned also works with data that cluster in other ways around the regression line. More importantly, we'll also learn how to evaluate the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNBBu3y8BEV4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038TKGcH4MvR"
   },
   "source": [
    "## Root Mean Squared Error\n",
    "\n",
    "First we look at the dataset from the textbook, which works with data from the _Little Women_ novel that we used in Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "CHdHIW5KOIVt",
    "outputId": "cbaa86e2-b13c-4416-9957-3ee219c3a087"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/little_women.csv'\n",
    "little_women = pd.read_csv(url)\n",
    "print(\"Number of rows or chapters:\", len(little_women))\n",
    "print(\"First 5 rows:\")\n",
    "little_women.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTd4CKcQLXcL"
   },
   "source": [
    "Each row of the dataset is for one chapter in the book. The row contains the number of characters (letters, numbers, punctuation, spaces) and the number of periods in the chapter.\n",
    "\n",
    "We want to see if we can predict the number of characters from the number of periods. This means the `x` data is the number of periods, and the  $\\hat{y}$ data is the number of predicted characters.\n",
    "\n",
    "We plot the number of periods vs the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "O5-rgh1YSLEg",
    "outputId": "2e098e87-72cd-4671-b00a-61bac8d9ed1a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(little_women['Periods'], little_women['Characters'])\n",
    "plt.xlabel('Periods')\n",
    "plt.ylabel('Characters')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuQ3THhGSVml"
   },
   "source": [
    "We see that the data values are not uniformly clustered around a regression line, but it does look like a linear, positive correlation.\n",
    "\n",
    "We find the correlation coefficient `r` by bringing in the `standard_units` and `correlation` functions from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgVwWMS4Tzyz",
    "outputId": "4e9c4312-ba5c-4ad3-ad90-90f168cc8bbe"
   },
   "outputs": [],
   "source": [
    "def standard_units(array):\n",
    "    return (array - np.mean(array))/np.std(array)\n",
    "\n",
    "def correlation(array1, array2):\n",
    "    return np.mean(standard_units(array1) * standard_units(array2))\n",
    "\n",
    "r = correlation(little_women['Periods'], little_women['Characters'])\n",
    "print(\"r:\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2EMN97oT9h5"
   },
   "source": [
    "There is a high correlation of 0.92.\n",
    "\n",
    "Next we bring in the `slope` and `intercept` functions from the previous notebook to find the slope and intercept that define the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdU2FtEZ34C3",
    "outputId": "83cc7e4e-26d7-4c01-c963-3bfff322f210"
   },
   "outputs": [],
   "source": [
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y) * np.mean(x)\n",
    "\n",
    "book_slope = slope(little_women['Periods'], little_women['Characters'])\n",
    "print(\"slope:\", book_slope)\n",
    "book_intercept = intercept(little_women['Periods'], little_women['Characters'])\n",
    "print(\"intercept:\", book_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KkulQ8FfJNH"
   },
   "source": [
    "Recalling what the slope of the regression line means:\n",
    "> The slope shows that on average, there are about 87 characters for every period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmWvq-eSTYB4"
   },
   "source": [
    "We plot the prediction line in the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "Ar1mo9inVYw4",
    "outputId": "2a8e9389-a0bf-4651-f306-5490da9d5bd6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(little_women['Periods'], little_women['Characters'])\n",
    "plt.plot(little_women['Periods'], book_slope * little_women['Periods'] + book_intercept, color='green')\n",
    "plt.xlabel('Periods')\n",
    "plt.ylabel('Characters')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mrh9np3048J"
   },
   "source": [
    "As we noted in the previous notebook, the regression line is the best fit line, it doesn't go through all the data points.\n",
    "\n",
    "In the plot above, the blue points are the actual `y` values or the actual number of characters, and the green regression line is the $\\hat{y}$ values or the predicted number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu_FOyYwVkI4"
   },
   "source": [
    "We now calculate the predicted number of characters (the $\\hat{y}$ value) from the number of periods (the `x` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MLr2dyqV6kR"
   },
   "outputs": [],
   "source": [
    "Predicted_Characters = np.round(book_slope * little_women['Periods'] + book_intercept, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyhZQOAWGBI"
   },
   "source": [
    "Then we find the difference or error between the predicted_characters and the actual number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTU5o0eXWPOL"
   },
   "outputs": [],
   "source": [
    "Error = little_women['Characters'] - Predicted_Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrgS-y8bWdsd"
   },
   "source": [
    "Last we add these 2 results into the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "XiGfWeEUWmkd",
    "outputId": "f558f9e8-a976-4f5f-8927-5a7aee53c551"
   },
   "outputs": [],
   "source": [
    "little_women['Predicted_Characters'] = Predicted_Characters\n",
    "little_women['Error'] = Error\n",
    "print(\"First 5 rows:\")\n",
    "little_women.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1L6XmtzY-l9"
   },
   "source": [
    "Now we plot the error for some of the data points on the scatterplot to illustrate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "MV2N8Ri2ZZrI",
    "outputId": "ac10a944-52fb-4e25-83b2-ea8588fbe25d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(little_women['Periods'], little_women['Characters'])\n",
    "plt.plot(little_women['Periods'], book_slope * little_women['Periods'] + book_intercept, color='green')\n",
    "plt.xlabel('Periods')\n",
    "plt.ylabel('Characters')\n",
    "plt.grid()\n",
    "\n",
    "# You don't need to write the code below.\n",
    "# This code shows the error on the plot for discussion purpose.\n",
    "\n",
    "# Select 5 data points\n",
    "selected_points = little_women.iloc[[22, 0, 40, 24, 46]]\n",
    "# Add vertical lines for the error\n",
    "for index, row in selected_points.iterrows():\n",
    "    plt.plot([row['Periods'], row['Periods']], [row['Characters'], row['Predicted_Characters']], color='red', linestyle='-', linewidth=1.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5iSESYOcnup"
   },
   "source": [
    "We see that out of 5 data points that we chose randomly, 4 are a distance away from the linear progression line, as shown in red. The red lines are the `Error` values asssociated with these data points. The 5th data point is on the regression line or is so close to it such that there's no red error line shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txd1x3Da_PRp"
   },
   "source": [
    "Since most data points are some distance away from the regression line, we want to find their average distance from the line.\n",
    "\n",
    "To do this, we don't want to do a simple averaging, which is to add the `Error` values and divide the sum by the total number of data values. This is because some of the `Error` values are positive and others are negative. If we add the positives and negatives together, they effectively \"cancel\" each other out, giving us a false average error that might be too low. For example, if we have -5 and +5 as the `Error` values for 2 data points, if we add them together we'll end up with 0 average error.\n",
    "\n",
    "Instead we'll use the same method as the calculation of the standard deviation:\n",
    "- square the error values to remove the negative sign\n",
    "- find the average of all the squares\n",
    "- take the square root to 'undo' the squaring of the first step\n",
    "\n",
    "The resulting average error is called the root mean squared error or RMSE. The long name describes how the errors are processsed: squared, then take their mean, and take the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTTqeG0x28zs",
    "outputId": "23147fa3-1afe-4ffb-c51e-9fdad60892ae"
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(little_women['Error']**2))\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Average number of characters:\", np.mean(little_women['Characters']))\n",
    "print(\"Relative RMSE percent:\", round(rmse / np.mean(little_women['Characters']) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-k8xFBF2vc6"
   },
   "source": [
    "The error in prediction is about $\\pm 2700$ characters out of an average of about 21,700 characters per chapter. This is about 12% error.\n",
    "\n",
    "In general, a rough guideline for relative RMSE error is:\n",
    "- < 10%: excellent\n",
    "- 10 - 20%: good\n",
    "- 20 - 30%: fair\n",
    "- \\> 30%: poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajAAW6gEi16f"
   },
   "source": [
    "Mathematically, among all straight lines that can be drawn or calculated to estimate the $\\hat{y}$ values, the regression line is the unique line that minimizes the root mean squared error. This is why the regression line is also called the _least squares_ line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MxbkViWiLw9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWQnfF9ZiNSh"
   },
   "source": [
    "## The Residuals\n",
    "\n",
    "Since the linear regression line is the best-fit line and not an exact-fit line, usually there is a distance between the actual `y` value and the predicted $\\hat{y}$ value, which is considered the error in the prediction.\n",
    "\n",
    "The error in the prediction is also called the _residual_. In the _Little Women_ scatterplot above, the residuals are the red lines shown in the plot. While we can plot the residual for each data point with a red line, it's easy to imagine that all the red lines from all the data points would overlap each other, making it difficult to make good observations of them.\n",
    "\n",
    "Instead, we can draw residual plots to have a better visualization of the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06HP4FCEhcTw"
   },
   "source": [
    "We start with the same dataset of the parent and child heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1RoljsU4hmKD",
    "outputId": "a926f6a4-e649-4ac9-bf53-281a40450d5d"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/galton.csv\"\n",
    "family = pd.read_csv(url)\n",
    "display(family.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxJYFoq3jCJ2"
   },
   "source": [
    "Just as in the previous dataset, we create a new DataFrame with the `midparentHeight` and `childHeight` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "99sTwo12jfmu",
    "outputId": "db974d46-60a4-49cf-ac69-70341e8ba6f3"
   },
   "outputs": [],
   "source": [
    "heights = family.loc[:, ['midparentHeight', 'childHeight']].copy()\n",
    "display(heights.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTnfhydvjvTh"
   },
   "source": [
    "Then we bring in the `slope` and `intercept` function from the previous notebook and use them to build a new `fit` function that produces the predicted child height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85IVB-ZfkMd8"
   },
   "outputs": [],
   "source": [
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y) * np.mean(x)\n",
    "\n",
    "def fit(x, y):\n",
    "    return slope(x, y) * x + intercept(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0Ly1v70kflj"
   },
   "source": [
    "Using the `fit` function, we add 2 new columns for the `heights` DataFrame: predictHeight and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XGE6Lx9xk0MX",
    "outputId": "553a9ac7-39bf-4f30-9779-074438ea3384"
   },
   "outputs": [],
   "source": [
    "heights['predictHeight'] = fit(heights['midparentHeight'], heights['childHeight'])\n",
    "heights['Error'] = heights['childHeight'] - heights['predictHeight']\n",
    "display(heights.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6kL7S_BlF56"
   },
   "source": [
    "We now plot the `midParentHeight` vs the actual `childHeight` in a scatterplot, and then add the linear regression line from the `predictHeight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ZTnpiCfwlWGK",
    "outputId": "b079fc0e-8f5f-4ba9-a7a2-f3b4cb67926d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(heights['midparentHeight'], heights['childHeight'], alpha=0.6)\n",
    "plt.plot(heights['midparentHeight'], heights['predictHeight'], color='green', linewidth=2)\n",
    "plt.xlabel('Midparent Height')\n",
    "plt.ylabel('Child Height')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtvbqMEamAi5"
   },
   "source": [
    "It's easy to imagine that if we plotted a red error line from each blue data point to the regression line, it would be difficult to see the errors or residuals.\n",
    "\n",
    "Instead, we use a residual plot, which is a plot of the `midparentHeight` (`x` values) vs the `Error`($\\hat{y}$ values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "f4PNYROXnMsP",
    "outputId": "cfc4681a-25ba-4c5e-c242-30ed3d7ba981"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(heights['midparentHeight'], heights['Error'], alpha=0.6, color='firebrick')\n",
    "plt.plot(heights['midparentHeight'], np.zeros_like(heights['midparentHeight']),\n",
    "         color='purple', linewidth=2)\n",
    "plt.xlabel('Midparent Height')\n",
    "plt.ylabel('Error or Residual')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oRyGssSntgF"
   },
   "source": [
    "We make the following observations, which are typical of a good linear regression:\n",
    "-  The residuals are centered around 0 on the y-axis. This makes sense since 0 error is the ideal.\n",
    "- The spread is about the same above and below the horizontal line at y=0, for the majority of x values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dKbljODESPX"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ZkO3CfraRr"
   },
   "source": [
    "### Use Case for Residual Plots\n",
    "\n",
    "**Detecting Non Linear Correlation**\n",
    "\n",
    "Residual plots can be used to detect when a correlation is not linear.\n",
    "\n",
    "We use the same dataset as the textbook, which works with the age and length of a sea mammal called a dugong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "byBd0OCYz0xi",
    "outputId": "267e48af-8aec-4204-da26-a9d61747b31f"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/dugong.csv\"\n",
    "dugong = pd.read_csv(url)\n",
    "display(dugong.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYBXKGxF0gsU"
   },
   "source": [
    "As the dugong's age from birth gets higher, we expect its size or length measurement to be higher also. If we can measure the length of a dugong, can we predict its age?\n",
    "\n",
    "We display the plot for the `Length` vs `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "Jz1FmSXx1i4n",
    "outputId": "c35d2deb-6638-4d58-fbc7-f127f8647fe2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(dugong['Length'], dugong['Age'], alpha=0.6)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Age')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85-DeIhh6c3w"
   },
   "source": [
    "It looks like there is mostly positive correlation, but it might be difficult to fit a line along the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn48oIUO1o4t"
   },
   "source": [
    "We calculate the regression line and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "YRUhMuZs6Nvq",
    "outputId": "752a5db5-7ff1-4e40-b4ce-ba14667579e8"
   },
   "outputs": [],
   "source": [
    "predicted_length = fit(dugong['Length'], dugong['Age'])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(dugong['Length'], dugong['Age'], alpha=0.6)\n",
    "plt.plot(dugong['Length'], predicted_length, color='green', linewidth=2)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Age')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_zjiDlz6Yp6"
   },
   "source": [
    "It looks like there are errors at the larger lengths, and in the middle range of length, the majority of the data points are below the regression line.\n",
    "\n",
    "We look at the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "o0-kRotc8N4v",
    "outputId": "eeb476ea-7d7d-48cc-91c6-1fbe0de013f9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(dugong['Length'], dugong['Age'] - predicted_length, alpha=0.6, color='firebrick')\n",
    "plt.plot(dugong['Length'], np.zeros_like(dugong['Length']), color='purple', linewidth=2)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Error or Residual')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n6R6TOJ8XkQ"
   },
   "source": [
    "We can see that across the `Length`, the spread is not the same above and below the y=0 line.\n",
    "- The residuals are above the y=0 line for the lowest lengths.\n",
    "- They mostly drop below the line in the middle range.\n",
    "- They are above the line again at the highest lengths.\n",
    "\n",
    "This means the regression is not a line but more of a curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awAhU974xvbL"
   },
   "source": [
    "**Detecting Uneven Spread**\n",
    "\n",
    "Residual plots can be used to detect uneven spread of errors, such as when the errors are higher for a specific range of data in the dataset. The term for uneven spread is _heteroscedasticity_, so we can say that residual plots can be used to detect heteroscedasticity.\n",
    "\n",
    "We use the dataset of hybrid cars in the US that we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GPmiIBVktYqk",
    "outputId": "a256657e-50f8-4ea5-eb4a-f2805566554d"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/hybrid.csv\"\n",
    "hybrid = pd.read_csv(url)\n",
    "display(hybrid.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSJOSBVFta8Z"
   },
   "source": [
    "We plot the `mpg` vs `acceleration` and recall that it's not a linear correlation: the `mpg` drops as `acceleration` capability increases, but the `mpg` drop flattens out at the highest `acceleration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "IlkzR-ADt-79",
    "outputId": "cbece1ec-0c8d-425e-bbbd-42d5102c0212"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(hybrid[\"acceleration\"], hybrid[\"mpg\"], alpha=0.6)\n",
    "plt.xlabel(\"acceleration\")\n",
    "plt.ylabel(\"mpg\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms4TwbVguj5Y"
   },
   "source": [
    "We calculate the linear regression line and add it to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "6Lo47-Fhu3zm",
    "outputId": "6f04e8cf-86e9-458a-b4bf-ac6e1e3bee0b"
   },
   "outputs": [],
   "source": [
    "linear_regression_line = fit(hybrid[\"acceleration\"], hybrid[\"mpg\"])\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(hybrid[\"acceleration\"], hybrid[\"mpg\"], alpha=0.6)\n",
    "plt.plot(hybrid[\"acceleration\"], linear_regression_line, color='green', linewidth=2)\n",
    "plt.xlabel(\"acceleration\")\n",
    "plt.ylabel(\"mpg\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5Amqp0avC7q"
   },
   "source": [
    "Observing the plot above, we can tell that the linear regression line doesn't fit the data points very well at the low end of the acceleration capability.\n",
    "\n",
    "The residual plot agrees with our observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "IgUJVjsdvazD",
    "outputId": "8bff7275-ce10-4cdf-fa7e-3c6666c66e5d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(hybrid[\"acceleration\"], hybrid[\"mpg\"] - linear_regression_line,\n",
    "            alpha=0.6, color='firebrick')\n",
    "plt.plot(hybrid[\"acceleration\"], np.zeros_like(hybrid[\"acceleration\"]),\n",
    "         color='purple', linewidth=2)\n",
    "plt.xlabel(\"acceleration\")\n",
    "plt.ylabel(\"Error or Residual\")\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_FIaLlzv2pb"
   },
   "source": [
    "Sure enough, the residual plot shows that the residual spread is not the same above and below 0 for all acceleration capabilities. There is more error or more spread at the lower end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNOXvcniE8b-"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPJsfEj4-Wde"
   },
   "source": [
    "## Properties of the Residuals\n",
    "\n",
    "1. <u>The residuals and the predicted variables are not correlated.</u><br>\n",
    "This means that the differences between actual data and predicted data (the residuals) are random and contain no pattern related to the predicted values.\n",
    "\n",
    "We check the above property by finding the correlation between the residuals and predicted values of the 3 examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YMbj4UhDEmB"
   },
   "source": [
    "First we find the correlation of the predicted child's height `predictHeight` and its residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2r2L6eNDxZq",
    "outputId": "4f431c47-fbb9-4be6-dcf9-5e91639adb22"
   },
   "outputs": [],
   "source": [
    "print(\"r = \", round(correlation(heights['predictHeight'], heights['Error']), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzv8miYhEDnu"
   },
   "source": [
    "Likewise, we find the correlation of the dugong's `predicted_length` and its residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yTYFzT8Ei8k",
    "outputId": "8257d421-d623-49e4-c6da-c2d0f1bffa20"
   },
   "outputs": [],
   "source": [
    "print(\"r = \", round(correlation(predicted_length, dugong['Age'] - predicted_length), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-xQhj4QE3TG"
   },
   "source": [
    "And we find the correlation of the car's `mpg` and its residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tlqxQ-1E-Kl",
    "outputId": "58a66c70-f1f4-4d12-9610-26933cab605d"
   },
   "outputs": [],
   "source": [
    "print(\"r =\", round(correlation(linear_regression_line, hybrid[\"mpg\"] - linear_regression_line), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP_MuXyMFJpO"
   },
   "source": [
    "2. <u>The average of the residuals is 0.</u><br>\n",
    "Since the linear regression line is the best-fit line, the positive differences between actual and predicted values and the negative differences between actual and predicted values will 'cancel' each other out. Therefore their sums will be 0.\n",
    "\n",
    "We check the above property with the 3 examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuHeVl9_KU7b",
    "outputId": "fa808e78-b35c-4f15-8b72-4b3a32ad288e"
   },
   "outputs": [],
   "source": [
    "print(\"Average of the residuals for heights:\", round(np.mean(heights['Error']), 2))\n",
    "print(\"Average of the residuals for dugong:\", round(np.mean(dugong['Age'] - predicted_length), 2))\n",
    "print(\"Average of the residuals for cars:\", round(np.mean(hybrid[\"mpg\"] - linear_regression_line), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1wF45opK0VL"
   },
   "source": [
    "3. <u>The ratio of SD of residuals to SD of `y` is: <br>\n",
    "$$\n",
    "\\frac{SD_{residual}}{SD_{y}} =\n",
    "\\sqrt{1 - r^2}\n",
    "$$</u><br>\n",
    "where SD is the standard deviation and `r` is the correlation coefficient.<br>\n",
    "\n",
    "Looking at the \"perfect\" values for `r`, which is when `r` is 1 or -1, we can see that the ratio is true. Recall from the previous notebook that when `r` is 1 or -1, the data points line up in a straight line in the scatterplot.\n",
    "- On the left side of the equation: If the data line up in a straight line, it means that the regression line would fit perfectly with the data points and the SD of the residuals would be 0.<br>\n",
    "- On the right side of the equation: Evaluating the square root above with `r` being 1 or -1, the result is also 0.\n",
    "\n",
    "For actual `r` values that are between 0 and 1, we check the observed SD ratio with the 3 datasets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu63oj14TVxo",
    "outputId": "95e1fdde-9c0a-410e-9ba3-015724eef5df"
   },
   "outputs": [],
   "source": [
    "# predict child height from parent height\n",
    "\n",
    "SD_y = np.std(heights['childHeight'])\n",
    "SD_residual = np.std(heights['Error'])\n",
    "r = correlation(heights['midparentHeight'], heights['childHeight'])\n",
    "print(\"Heights\")\n",
    "print(\"SD ratio:\", round(SD_residual/SD_y, 4))\n",
    "print(\"square root:\", round(np.sqrt(1 - (r**2)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpRxQ5J7VEs_",
    "outputId": "06cc7244-4682-4448-8b14-9d6a6a42d087"
   },
   "outputs": [],
   "source": [
    "# predict dugong age from length\n",
    "\n",
    "SD_y = np.std(dugong['Age'])\n",
    "SD_residual = np.std(dugong['Age'] - predicted_length)\n",
    "r = correlation(dugong['Length'], dugong['Age'])\n",
    "print(\"Dugong\")\n",
    "print(\"SD ratio:\", round(SD_residual/SD_y, 4))\n",
    "print(\"square root:\", round(np.sqrt(1 - (r**2)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TUUJdaeVGyl",
    "outputId": "acf6d8d5-9d5e-4fca-e971-eca9b48ef73a"
   },
   "outputs": [],
   "source": [
    "# predict hybrid car mpg from acceleration\n",
    "\n",
    "SD_y = np.std(hybrid[\"mpg\"])\n",
    "SD_residual = np.std(hybrid[\"mpg\"] - linear_regression_line)\n",
    "r = correlation(hybrid['acceleration'], hybrid[\"mpg\"])\n",
    "print(\"Cars\")\n",
    "print(\"ratio:\", round(SD_residual/SD_y, 4))\n",
    "print(\"square root:\", round(np.sqrt(1 - (r**2)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feIBVbd3ZMj8"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrYW4s-vZOHx"
   },
   "source": [
    "In this notebook we learn that when 2 features of a dataset have linear correlation, then a linear regression line can be calculated to best fit the correlated data. This regression line is the line of predicted data.\n",
    "\n",
    "The regression line is optimized so that the residuals, or the differences between predicted and actual data, are minimized and the prediction can be as accurate as possible.\n",
    "\n",
    "The ability to predict a characteristic of the dataset with reasonable accuracy is the data science foundation on which machine learning models are built. Moving beyond calculating the linear regression of a dataset, data scientists use the same simulation and estimation techniques that we've learned to build _linear regression models_ that work with any dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
