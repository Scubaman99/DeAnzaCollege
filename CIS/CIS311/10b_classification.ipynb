{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "\n",
        "## Reading\n",
        "\n",
        "[Chapter 17: 17.1 - 17.3](https://inferentialthinking.com/chapters/17/Classification.html)"
      ],
      "metadata": {
        "id": "xL37UwFdHMd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous two notebooks we studied a machine learning technique called linear regression, in which we write code to create a  regression line for the data. This regression line gives us the predicted values of a specific characteristic of the data.\n",
        "\n",
        "In this notebook and the next notebook we investigate another machine learning technique called _classification_. With classification we work with data that are made of two or more distinct *groups* or *classes*. An example would be data for patients who have cancer or don't have cancer, and we use the patient's data to _classify_ or predict whether the patient has cancer or not.\n",
        "\n",
        "When the predicted data are in distinct groups or classes, they are _categorical_ data. We recall that we've worked with categorical data before, when we used a bar chart to plot data in Module 3 Plots class notes.\n",
        "\n",
        "For this introduction to classification, we will work with datasets where the predicted data are in 2 distinct classes. This is called _binary classification_."
      ],
      "metadata": {
        "id": "QKlWl1vsHkN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "sdVX26AqGUdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neighbor\n",
        "\n",
        "We look at one of the most common methods for classification called the _nearest neighbor_ method.\n",
        "\n",
        "But before we dive into creating a classification algorithm, we'll first investigate the common concepts in classification: nearest neighbor, decision boundary, and k nearest neighbors.\n",
        "\n",
        "\n",
        "We start with the same dataset as the textbook, which works with data for chronic kidney desease or CKD."
      ],
      "metadata": {
        "id": "QdbkaCNFLFPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/ckd.csv'\n",
        "ckd = pd.read_csv(url)\n",
        "print(\"First 5 rows:\")\n",
        "ckd.head()"
      ],
      "metadata": {
        "id": "mJsfzKbF13US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row has data for one patient, and the patient's data (the columns) are measurements of different blood test markers.\n",
        "\n",
        "For our study of classification, we'll take a look at these columns: `Hemoglobin`, `White Blood Cell Count`, `Blood Glucose Random`, and `Class`.\n",
        "\n",
        "The `Class` variable has 2 values: 0 and 1, which represents the patient status of no CKD (0) or CKD (1)."
      ],
      "metadata": {
        "id": "WfJnbGuM3T8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDbgqMZNmVGZ"
      },
      "outputs": [],
      "source": [
        "ckd.Class.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a new DataFrame from the 4 selected columns, and shorten the name `Blood Glucose Random` to `Glucose`."
      ],
      "metadata": {
        "id": "fnJPEf9T51rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ckd[['Hemoglobin', 'White Blood Cell Count', 'Blood Glucose Random', 'Class']]\n",
        "data = data.rename(columns={'Blood Glucose Random': 'Glucose'})\n",
        "print(\"First 5 rows:\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "JcQGRa1m6GqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the blood markers are in different units of measurements, we change all 3 markers to standard units so that they all have the same uniform units.\n",
        "\n",
        "We bring in the `standard_units` function from the Module 8 notebook, then we use the function to convert the data."
      ],
      "metadata": {
        "id": "Y2a24KCq6hlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_units(array):\n",
        "    return (array - np.mean(array))/np.std(array)\n",
        "\n",
        "data['Hemoglobin'] = standard_units(data['Hemoglobin'])\n",
        "data['White Blood Cell Count'] = standard_units(data['White Blood Cell Count'])\n",
        "data['Glucose'] = standard_units(data['Glucose'])\n",
        "print(\"First 5 rows in standard units:\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DjGfrOvm64J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we look at the relationship between `Hemoglobin` and `Glucose` by plotting them in a scatterplot."
      ],
      "metadata": {
        "id": "NacTtV7e7ppI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(data['Hemoglobin'], data['Glucose'])\n",
        "plt.xlabel('Hemoglobin')\n",
        "plt.ylabel('Glucose')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oxGi6UJv9dya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appears to be no correlation between these 2 markers, and especially not linear correlation.\n",
        "\n",
        "But we also have one more patient feature that we can use: we can group the data by `Class`. This allows us to see the `Glucose` and `Hemoglobin` for CKD and non-CKD patients."
      ],
      "metadata": {
        "id": "0a3Fui3b9Zo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "\n",
        "# group by CKD or non-CKD\n",
        "groups = data.groupby('Class')\n",
        "\n",
        "# scatterplot for each group\n",
        "for name, group in groups:\n",
        "    plt.scatter(group['Hemoglobin'], group['Glucose'], label=name)\n",
        "\n",
        "plt.xlabel('Hemoglobin')\n",
        "plt.ylabel('Glucose')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3zrujyV8Rv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we clearly see a pattern with the 2 groups or classes:\n",
        "- Patients in class 0 or non-CKD have low `Glucose` level between -1 and 0 standard units, and high `Hemoglobin` level between -0.5 to 1.5 standard units. Their data are clustered together.\n",
        "- Patients in class 1 or CKD have a wide range of `Glucose` level, and a`Hemoglobin` level that's mostly less than 0."
      ],
      "metadata": {
        "id": "narOiOM--tLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have a patient named Alice, to use the same name as in the textbook, and we know her `Glucose` and `Hemoglobin` levels, then there's a good chance we can plot her data in the scatterplot and determine whether Alice has CKD or not.\n",
        "\n",
        "- If Alice's data placed her near or in the blue cluster of data points above, then we would conclude that she's in the non-CKD group.\n",
        "- If her data placed her near some of the orange data points and far away from the blue cluster, then we would conclude that she's in the CKD class.\n",
        "\n",
        "The reasoning above is an example of the nearest neighbor classification method. We predict the class for the new data based on the group of the _closest_ existing data. The idea is that when 2 data values are close together, they are likely to share the same characteristics."
      ],
      "metadata": {
        "id": "3lnslVEy-s8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "9Wpy_eJJV0Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Boundary\n",
        "\n",
        "If Alice's data placed her very close to the blue cluster or very far from the blue cluster, then the CKD or non-CKD decision is staightforward.\n",
        "\n",
        "But what if Alice is placed somewhere close to both the orange and blue data points?\n",
        "\n",
        "We now give Alice a `Glucose` and a `Hemoglobin` level as shown below."
      ],
      "metadata": {
        "id": "gmz0aoBwDzmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "\n",
        "# group by CKD or non-CKD\n",
        "groups = data.groupby('Class')\n",
        "\n",
        "# scatterplot for each group\n",
        "for name, group in groups:\n",
        "    plt.scatter(group['Hemoglobin'], group['Glucose'], label=name)\n",
        "\n",
        "# add Alice\n",
        "plt.scatter(0, 1, color='purple', label='Alice')\n",
        "# 0 is the value for x or Hemoglobin\n",
        "# 1 is the value for y or Glucose\n",
        "\n",
        "plt.xlabel('Hemoglobin')\n",
        "plt.ylabel('Glucose')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rG5J57EvFHo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's not so easy to visualize which class Alice belongs. Her data point looks close to both the orange and the blue data points.\n",
        "\n",
        "Fortunately we can calculate the distance between the purple data point (Alice) and the orange and blue data points (existing patients in the dataset).\n"
      ],
      "metadata": {
        "id": "44OGtSqOFZAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You don't have to write the code below at this time.\n",
        "# It's used to demo the nearest neighbor concept.\n",
        "\n",
        "# The code finds the minimum distance between Alice's data point and\n",
        "# other patients' data points, then it prints the data of the patient\n",
        "# closest to Alice.\n",
        "\n",
        "# Alice's data point\n",
        "alice = np.array([0, 1])\n",
        "\n",
        "# function to calculate the distance between 2 points\n",
        "def find_distance(point1, point2):\n",
        "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
        "\n",
        "# find distance from Alice to each data point in the DataFrame\n",
        "data['Distance_to_Alice'] = data.apply(lambda row: find_distance(alice,\n",
        "                                  [row['Hemoglobin'], row['Glucose']]), axis=1)\n",
        "\n",
        "# find the data point with the minimum distance\n",
        "closest_point = data.loc[data['Distance_to_Alice'].idxmin()]\n",
        "\n",
        "print(\"Closest data point to Alice:\")\n",
        "print(\"(x,y) =\", round(closest_point['Hemoglobin'],2), \",\",\n",
        "      round(closest_point['Glucose'], 2))\n",
        "print(\"Distance to Alice:\", round(closest_point['Distance_to_Alice'], 2))\n",
        "print(\"Class:\", closest_point['Class'])"
      ],
      "metadata": {
        "id": "x-R-gE9pF19x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result shows that the closest patient to Alice is the orange data point to the right of Alice's data point. Since this data point has `Class` 1, we conclude that Alice is also in class 1, which  means Alice likely has CDK.\n",
        "\n"
      ],
      "metadata": {
        "id": "CUoJB6T-H7U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous example, Alice's data point is at (0, 1), which results in the conclusion that Alice is in the CDK class. What if Alice's data point is at (0, 0.5) instead?\n",
        "\n",
        "We can use the same code as above to find the class in which she belongs."
      ],
      "metadata": {
        "id": "Q9AnRO8zgy47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alice's new data point\n",
        "alice = [0, 0.5]\n",
        "\n",
        "data['Distance_to_Alice'] = data.apply(lambda row: find_distance(alice, [row['Hemoglobin'], row['Glucose']]), axis=1)\n",
        "closest_point = data.loc[data['Distance_to_Alice'].idxmin()]\n",
        "\n",
        "print(\"Closest data point to Alice:\")\n",
        "print(\"(x,y) =\", round(closest_point['Hemoglobin'],2), \",\", round(closest_point['Glucose'], 2))\n",
        "print(\"Distance to Alice:\", round(closest_point['Distance_to_Alice'], 2))\n",
        "print(\"Class:\", closest_point['Class'])"
      ],
      "metadata": {
        "id": "rkIfDtm-iBIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the closest data point is a blue data point that appears below Alice's data point in the plot. And since that blue data point has `Class` 0, it means Alice likely belongs in the non-CDK class."
      ],
      "metadata": {
        "id": "HumlC-67iSgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Alice's location on the plot determines her outcome of CDK or non-CDK. The location (x, y) where Alice switches from being in the CDK class to being in the non-CDK class is part of the _decision boundary_.\n",
        "\n",
        "The decision boundary is the dividing line between 2 classes of data: each side of the decision boundary are data for one of the 2 classes. For example, one side of the decision line is for the CDK class, and any data that is placed on that side will be determined to be in the CDK class. Conversely, any data that is placed on the other side of the decision line will be determined to be in the non-CDK class. The [textbook](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html#decision-boundary) shows the plot of the decision boundary for the `Hemoglobin` vs `Glucose` scatterplot."
      ],
      "metadata": {
        "id": "wXd9I67Silu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "a81vjPI3lNUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Nearest Neighbors\n",
        "\n",
        "When the scatterplot shows 2 distinct classes, such as the `Hemoglobin` vs `Glucose` scatterplot, then it is more straightforward to find the nearest neighbor so that we can use its `Class` to determine our data's `Class`.\n",
        "\n",
        "But the sometimes the groupings overlap, such as when we look at `White Blood Cell Count` and `Glucose`."
      ],
      "metadata": {
        "id": "n6tBx62UlPLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "\n",
        "groups = data.groupby('Class')\n",
        "for name, group in groups:\n",
        "    plt.scatter(group['White Blood Cell Count'], group['Glucose'], label=name)\n",
        "\n",
        "plt.xlabel('White Blood Cell Count')\n",
        "plt.ylabel('Glucose')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6oaFS1Bmhs28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If Alice's data point is somewhere in the cluster of blue dots, we don't have confidence in which class she belongs.  "
      ],
      "metadata": {
        "id": "RzcBbR3Bo-lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we use a technique called the _k Nearest Neighbors_. Instead of looking for the one closest neighbor, we look at _k_ of the closest neighbors. If these neighbors are not all in the same class, then we'll go with the class of the majority of the k neighbors. For example, out of the nearest 3 neighbors, if 2 of them are in the non-CDK class then we conclude that Alice is also in the non-CDK class."
      ],
      "metadata": {
        "id": "V5cYh_copsLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mH9qbOOCqz6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing\n",
        "\n",
        "An important concept in machine learning prediction is the _training_ of the algorithm to do the prediction and then _testing_ the algorithm to see how accurate the predictions are.\n",
        "\n",
        "The algorithm to do the prediction is the sequence of steps it takes to find the nearest neighbor(s). In the previous examples with Alice's data point, the prediction algorithm is the code that:\n",
        "- finds the  distance between Alice's data point and all existing patients' data points\n",
        "- pinpoints the nearest patient's or k patients' data point(s)"
      ],
      "metadata": {
        "id": "0N-TxQ5zq1jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Dataset and Testing Dataset\n",
        "\n",
        "To fully test how well a prediction algorithm works, we need to test it with many new data points or many \"Alice\" data points that the algorithm has not seen in the dataset. The standard practice is to take the given dataset and divide it into 2 separate (non-overlapping) parts: a training dataset and a testing dataset.\n",
        "\n",
        "<u>Training Dataset</u><br>\n",
        "First we run the algorithm with the data in the training dataset. For the CDK example above, the k nearest neighbor algorithm uses the training data to map the relationship between the `Glucose` and `White Blood Cell Count` for each `Class` group, just like with the scatterplot above.\n",
        "\n",
        "In machine learning terminology, when we run an algorithm with data that have all relevant features so that the algorithm can map out the data, it is said that we're _training_ the algorithm, and the algorithm _learns_ from the data.\n",
        "\n",
        "<u>Testing Dataset</u><br>\n",
        "After the algorithm has mapped the training data, then we take the testing dataset and remove the `Class` column. In the CDK example above, the testing data is like the \"Alice\" data point, which has the `Glucose` and `White Blood Cell Count` data but no `Class` data. Then the algorithm finds the k nearest neighbors for each of the testing data points and produces the predicted values.\n",
        "\n",
        "To measure the accuracy of the prediction, we compare the predicted values with the `Class` column that we removed from the testing dataset."
      ],
      "metadata": {
        "id": "tF3dXxhEuyFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "QSR0oQtJSteJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Training and Testing Datasets\n",
        "\n",
        "We use random sampling to create the 2 sets. First we shuffle all the rows of the dataset, then we choose the first half of the rows for the training set, and the second half of the rows for a testing set.\n",
        "\n",
        "From Module 7 AB Testing class notes, we shuffle by sampling all of the dataset, which means we set the `frac` or fraction of the dataset to 1."
      ],
      "metadata": {
        "id": "SjDQGXKbVf8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = data.sample(frac=1)\n",
        "# take first half\n",
        "training_set = shuffled_data.iloc[:len(shuffled_data)//2]\n",
        "# take second half\n",
        "testing_set = shuffled_data.iloc[len(shuffled_data)//2:]"
      ],
      "metadata": {
        "id": "xSswMEveWyrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We confirm that the training dataset still has the same characteristic as the original dataset by plotting the `Glucose` vs `White Blood Cell Count`."
      ],
      "metadata": {
        "id": "lW-huOM0cgXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "\n",
        "groups = training_set.groupby('Class')\n",
        "for name, group in groups:\n",
        "    plt.scatter(group['White Blood Cell Count'], group['Glucose'], label=name)\n",
        "\n",
        "plt.xlabel('White Blood Cell Count')\n",
        "plt.ylabel('Glucose')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FnSVwMeJVwwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are fewer data points than in the original dataset, but the non-CDK group is still clustered in the lower right hand corner, and the CDK group is still spread out throughout the plot, including having some data points inside the blue cluster."
      ],
      "metadata": {
        "id": "4L4hCJ6FdWEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "u9xZG6XzeHPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distance Calculation\n",
        "\n",
        "Now that we've explored the nearest neighbor classification algorithm and the concept of training and testing datasets, we will learn some new math and Python techniques to prepare for the next notebook, where we will code the nearest neighbor algorithm.\n",
        "\n",
        "One of the most important parts of the k nearest neighbor algorithm is to find the distance between two points, as that will help us find which training data points are the closest to the testing data point."
      ],
      "metadata": {
        "id": "CFf5akXPeIJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2-dimensional space, the distance between 2 points $(x_1, y_1)$ and $(x_2, y_2)$ is:\n",
        "$$\n",
        "D = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
        "$$\n",
        "\n",
        "The formula is derived from the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem), a foundational concept in geometry.\n",
        "\n",
        "The formula looks complicated but we'll break it down step by step in the code below. Then we'll plot it so we can see how the equation works."
      ],
      "metadata": {
        "id": "-55L58axtfpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create 2 points\n",
        "point_1 = (0, 0)   # (x1, y1)\n",
        "point_2 = (3, 4)   # (x2, y2)\n",
        "\n",
        "# Calculate the rise and run\n",
        "# - the rise is the difference along the y-axis between 2 points\n",
        "# - the run is the difference along the x-axis between 2 points\n",
        "run = point_1[0] - point_2[0]    # x1 - x2\n",
        "rise = point_1[1] - point_2[1]   # y1 - y2\n",
        "\n",
        "# Calculate the distance\n",
        "distance = np.sqrt(run**2 + rise**2)\n",
        "print(\"Distance between Point 1 and Point 2:\", distance)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(3,3))\n",
        "\n",
        "plt.plot(point_1[0], point_1[1], 'o', color='blue', label=\"Point 1\")\n",
        "plt.plot(point_2[0], point_2[1], 'o', color='orange', label=\"Point 2\")\n",
        "\n",
        "plt.plot([point_1[0], point_2[0]], [point_1[1], point_1[1]], color='purple',\n",
        "         linestyle='--', label=\"Run\")\n",
        "plt.plot([point_2[0], point_2[0]], [point_1[1], point_2[1]], color='green',\n",
        "         linestyle='--', label=\"Rise\")\n",
        "plt.plot([point_1[0], point_2[0]], [point_1[1], point_2[1]], color='black',\n",
        "         linestyle='-', label=\"Distance\")\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NAp2uCoosVDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that to calculate the distance between 2 points, we:\n",
        "- Find the run, which is the distance along the x-axis between the 2 points. In the plot above, it's the purple dashed line.\n",
        "- Find the rise, which is the distance along the y axis between the 2 points. In the plot above, it's the green dashed line.\n",
        "- Add the square of the rise and the square of the run, and take the square root of the sum. The result is the distance between the 2 points.\n",
        "\n",
        "We can write a function `find_distance` that goes through these steps, given the input point_1 and point_2."
      ],
      "metadata": {
        "id": "rsmtevbux_jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a point is an array with 2 values: [x, y]\n",
        "def find_distance(point_1, point_2):\n",
        "    run = point_1[0] - point_2[0]   # point[0] is the x component\n",
        "    rise = point_1[1] - point_2[1]  # point[1] is the y component\n",
        "    return np.sqrt(run**2 + rise**2)"
      ],
      "metadata": {
        "id": "5UEEYDQiy9F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EVazpTrv01QR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `apply` Function\n",
        "\n",
        "The DataFrame has an `apply` function that will  run a given function for the data in each row.\n",
        "\n",
        "<u>Example 1</u><br>\n",
        "We review the CDK `data` from above."
      ],
      "metadata": {
        "id": "-YMcE1td06Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"first 5 rows:\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "WV0vZR482Z-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that for each row, we want to find the higher of the `Hemoglobin` and the `Glucose` absolute values. For example, in the first row, the absolute value of `Hemoglobin` is 0.865744, and the absolute value of `Glucose` is 0.221549. The higher absolute value is 0.865744.\n",
        "\n"
      ],
      "metadata": {
        "id": "tPRN5gxe2fzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create a new DataFrame with the `Hemoglobin` and `Glucose` columns."
      ],
      "metadata": {
        "id": "7TQShKFI6fLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "two_columns = data[['Hemoglobin', 'Glucose']].copy()\n",
        "two_columns.head()"
      ],
      "metadata": {
        "id": "m_y3rlLF6nd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We write a function to find the higher absolute value."
      ],
      "metadata": {
        "id": "E6CHB_Oi6cZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def higher_abs_value(row):\n",
        "  return np.max(np.abs(row))"
      ],
      "metadata": {
        "id": "6AB1yauC5lll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run the `higher_abs_value` function with the first row to check that it produces the correct result."
      ],
      "metadata": {
        "id": "iA7_6jxu6_wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "higher_abs_value(two_columns.iloc[0])"
      ],
      "metadata": {
        "id": "GgvaO-V07D3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use the `apply` function to run a function with each row in the DataFrame, using the format:\n",
        "\n",
        "> `a_DataFrame.apply(function_name, axis=1)`\n",
        "\n",
        "The `axis=1` tells `apply` to run `function_name` with each _row_ of data. If we don't add the `axis=1`, then the default behavior for `apply` is to run the function with each column of the data.\n",
        "\n",
        "In the code below we run the `higher_abs_value` function with each row by using `apply` and we store the results as a new column in the `two_columns` DataFrame. Then we print the DataFrame to check that the output is correct."
      ],
      "metadata": {
        "id": "fMVlVDeG7fqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "two_columns['higher'] = two_columns.apply(higher_abs_value, axis=1)\n",
        "print(\"First 5 rows:\")\n",
        "two_columns.head()"
      ],
      "metadata": {
        "id": "mwQqZ6NR8Lcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Example 2</u><br>\n",
        "As a second example of using `apply`, given the function `find_distance` above, we write the function to find the distance between Alice and one data point in the `data` DataFrame.\n",
        "\n",
        "Alice's data point is already set at (1, 0.5) above."
      ],
      "metadata": {
        "id": "gcmJYzXX9Bua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_distance_to_alice(row):\n",
        "  return find_distance(alice, [row['Hemoglobin'], row['Glucose']])"
      ],
      "metadata": {
        "id": "OiCyZ7o3_PNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the function with the first row of `data`."
      ],
      "metadata": {
        "id": "nF_N1-No_02M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distance to Alice:\", find_distance_to_alice(data.iloc[0]))"
      ],
      "metadata": {
        "id": "SW-vFpIa_755"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the `apply` function to:\n",
        "- find the distance between Alice and all the  data points\n",
        "- store the distances in the variable named `distances`\n",
        "- find the smallest distance and print it"
      ],
      "metadata": {
        "id": "OKUa164m_zEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = data.apply(find_distance_to_alice, axis=1)\n",
        "print(\"Smallest distance:\", np.min(distances))"
      ],
      "metadata": {
        "id": "7fy_lFnu-rJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qUXaYLsjAxW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we learn the concepts of a classification problem and how the nearest neighbor method can be used to classify data.\n",
        "\n",
        "With this background knowledge, we will create a classification model in the next and final notebook."
      ],
      "metadata": {
        "id": "E952GAcSBWh8"
      }
    }
  ]
}