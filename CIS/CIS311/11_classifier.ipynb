{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Classifier\n",
        "\n",
        "## Reading\n",
        "\n",
        "[Chapter 17: 17.4, 17.5](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html)"
      ],
      "metadata": {
        "id": "vwG-f6Pk2bJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we learn to build a classifier that uses k nearest neighbors to classify data.\n",
        "\n",
        "In the previous notebook we saw that sometimes the groups in the dataset overlap each other in the scatterplot. The overlap makes it difficult to determine the class of a new dataset based on one closest neighbor, so we use k nearest neighbors and we go with the group that the majority of the k nearest neighbors have."
      ],
      "metadata": {
        "id": "E0rff4L42uO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Case for k Nearest Neighbors\n",
        "\n",
        "We work with the same dataset as in the textbook, which contains characteristics of different banknotes, such as a $10 bill. Using these characteristics we can determine whether a banknote is real or fake (counterfeit).\n",
        "\n",
        "We read in data to inspect them first."
      ],
      "metadata": {
        "id": "eHyHD6K-4Sfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQHk0Jbe2SXN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/banknote.csv'\n",
        "banknotes = pd.read_csv(url)\n",
        "print(\"First 5 rows:\")\n",
        "banknotes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The banknote attributes or features are in the first 4 columns. The last column is the `Class` column, which contains 0 for counterfeit and 1 for real money. Since there are 2 outcomes for `Class` this is a binary classification problem."
      ],
      "metadata": {
        "id": "Cc8N5mF7DqJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banknotes.Class.value_counts()"
      ],
      "metadata": {
        "id": "0UVT_rVJEJ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take a look at the scatterplot for `WaveletVar` and `WaveletCurt`."
      ],
      "metadata": {
        "id": "We1cnXo7EPTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "groups = banknotes.groupby('Class')\n",
        "for name, group in groups:\n",
        "    plt.scatter(group['WaveletVar'], group['WaveletCurt'], label=name, alpha=0.3)\n",
        "plt.xlabel('WaveletVar')\n",
        "plt.ylabel('WaveletCurt')\n",
        "plt.grid()\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "ghpTQzQEEXnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there's a region of overlap between the 2 classes, which means that it would be difficult to accurately predict the class for a new data value if it falls in the overlapping region. This is where using k neighbors could help. We would determine the class of the new data based on which class of the majority of the k nearest neighbors."
      ],
      "metadata": {
        "id": "FzKhvFO3FTly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "95Jm6_R8Fm0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Case for Multiple Attributes\n",
        "\n",
        "In the initial investigation of classification we've only looked at 2 features of the data at a time, but typically each data record has many features. We want to see if using more than 2 relevant features can help the classifier to be more accurate.\n",
        "\n",
        "Using the same `banknotes` dataset, we want to include all 3 attributes `WaveletVar`, `WaveletSkew`, and `WaveletCurt`.\n",
        "\n",
        "We use a 3D scatterplot to show the 3 attributes. Just like when we work with 2 attributes and plot the data on the 2D x-y plane, we plot the 3 attributes in 3D space with x, y, and z coordinates."
      ],
      "metadata": {
        "id": "W5Rm_z3TFsV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You don't need to write this code.\n",
        "# The code is used to demo the difference between using 2 attributes\n",
        "# and using 3 attributes of the banknotes data.\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "plt.figure(figsize=(6,7))\n",
        "# add 3D axes\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# group data by 'Class` and plot each group as before\n",
        "groups = banknotes.groupby('Class')\n",
        "for name, group in groups:\n",
        "    ax.scatter3D(group['WaveletSkew'], group['WaveletVar'], group['WaveletCurt'], label=name, alpha=0.4)\n",
        "\n",
        "ax.set_xlabel('WaveletVar')\n",
        "ax.set_ylabel('WaveletSkew')\n",
        "ax.set_zlabel('WaveletCurt')\n",
        "ax.legend()\n",
        "ax.view_init(elev=25, azim=290)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-j0IIh8EGDvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is almost no overlap in the 3D plot. When we used 2 features before, there was a region of overlap between the two clusters, which means the classifier could make a mistake when determining the group of the new data. But when we use 3 features, the two clusters have almost no overlap, making it more clear-cut which class the new data belongs. The third features's contribution is to separate the 2 clusters.\n",
        "\n",
        "In this case we see that a classifier that uses the 3 data features will be more accurate than a classifier that only uses 2 features."
      ],
      "metadata": {
        "id": "SG07wQlvWt3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if the data has 4, 5 or more relevant features? In general it's better to use all the relevant features because:\n",
        "- Each features can potentially help to define the clusters better, just like in the example above.\n",
        "- Each features describes the data in some way, and we don't want to miss important information about the data. This is similar to when we want to see if someone walking ahead of us is a friend. We don't want to just look at the person's hair and clothes from the back only, while ignoring their facial features.\n",
        "\n",
        "While using 4, 5 or more relevant features could mean better accuracy for our classifier, there is a downside:\n",
        "- We can no longer plot the features to see the clusters, since we can only plot up to 3D space.<br> However, even though we can no longer visualize the clusters, the mathematics and algorithm have no problem with multidimensional space. What works in 2D space can be extended to 5D or 12D space.\n",
        "- The computation cost for using multiple features will be higher since calculation with 2 features is simpler than calculation with 5 features. This is the reason why data scientists and machine learning / AI engineers need to work with powerful computers."
      ],
      "metadata": {
        "id": "oHcbNTOkZW55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but not least, it's also important to note that we only want to include _relevant_ features in the classification calculations. Adding unnecessary features will, at best, cause the computation cost to be higher, and at worst, confuse the classifier."
      ],
      "metadata": {
        "id": "NId-Ux-d73lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "FQcjYNiWe2gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Classifier\n",
        "\n",
        "We now have the background and tool necessary to create a simple classifier. We work with the dataset of wine features that is also used in the textbook."
      ],
      "metadata": {
        "id": "-DlyZRWhe4KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare Data"
      ],
      "metadata": {
        "id": "TFvbvJdcAVAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/wine.csv\"\n",
        "wine = pd.read_csv(url)\n",
        "rows, columns = wine.shape\n",
        "print(\"Number of rows:\", rows)\n",
        "print(\"Number of columns:\", columns)\n",
        "print(\"First 5 rows:\")\n",
        "wine.head()"
      ],
      "metadata": {
        "id": "M0z_vC5S217n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data in this dataset have 14 features and we'll use all of them. Our goal is to predict the type of wine, which is shown in the `Class` column. There are 3 `Class` values."
      ],
      "metadata": {
        "id": "2G8QwxrH-EMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine.Class.value_counts()"
      ],
      "metadata": {
        "id": "oGLR1_Xz-JgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are doing binary classification, we will concentrate on whether we can predict whether a wine is in class 1 or not in class 1.\n",
        "\n",
        "We'll change the `Class` column so that rows with `Class` value of 2 or 3 will end up with 0 for the value."
      ],
      "metadata": {
        "id": "xpO6RnIh-Nb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to change 2 or 3 into 0\n",
        "def change(x):\n",
        "    if x == 2 or x == 3:\n",
        "        return 0\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "# use apply with our change function to change the Class column\n",
        "wine['Class'] = wine['Class'].apply(change)\n",
        "wine.Class.value_counts()"
      ],
      "metadata": {
        "id": "p2vHJg7F_kMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the count for 1 is still 59, and the count for 0 is the sum of the counts of 2 and 3."
      ],
      "metadata": {
        "id": "qsTmUMkSACyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the previous notebook, we'll divide the dataset into training data and testing data. We use the same `shuffle` function that we wrote in the previous notebook."
      ],
      "metadata": {
        "id": "uU-VkK3BbUb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = wine.sample(frac=1)\n",
        "training_set = shuffled_data.iloc[:len(shuffled_data)//2].copy()\n",
        "testing_set = shuffled_data.iloc[len(shuffled_data)//2:].copy()"
      ],
      "metadata": {
        "id": "ioDiEXJ4bgKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take a look at the first 5 rows of the `training_set`."
      ],
      "metadata": {
        "id": "SvS2_x8kb8Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.head()"
      ],
      "metadata": {
        "id": "zPJ7e-ZBcG_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the rows have been shuffled randomly.\n",
        "\n",
        "We recall from Module 10 Classification class notes that the `training_set` will be used by the classification algorithm to find the association (the scatterplot equivalent) of all the features of the data.\n",
        "\n",
        "The `testing_set` will be used as the \"Alice\" data points for prediction."
      ],
      "metadata": {
        "id": "tCzhxedjbnEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hHAKxkVJHcFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Classification Algorithm\n",
        "\n",
        "To write the code for the classification algorithm, we follow the steps that we've used in Module 10 Classification class notes:\n",
        "1. Find the distance between the new data point and all the existing data points in the dataset.\n",
        "2. Use these distances to find the k points that are closest (the shortest distance) to the new data point.\n",
        "3. Find the class that occurs the most in the k nearest data points. This will be our predicted class for the new data point.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QI9Y7x-e83F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Find Distances\n",
        "\n",
        "<u>1a. Distance between the new data point and _one_ existing data point</u>\n",
        "\n",
        "We recall that the distance formula is:\n",
        "$$\n",
        "D = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
        "$$\n",
        "for 2 points in the 2D x-y plane.\n",
        "\n",
        "If the 2 points have multiple features, the distance formula still works in 4D, 5D, 6D or higher dimensional space. We simply extend the formula by continuing the subtraction of the corresponding values along the additional dimensions.\n",
        "\n",
        "For example, if the 2 points have 4 attributes, then the distance formula will be:\n",
        "$$\n",
        "D = \\sqrt{(x_1 - x_2)^2  + (y_1 - y_2)^2 + (z_1 - z_2)^2 + (w_1 - w_2)^2}\n",
        "$$\n",
        "\n",
        "Conveniently, numpy arrays come in handy in this situation. If the 2 points with 4 attributes are represented as 2 numpy arrays with 4 values in each array, then numpy will automatically subtract the corresponding values in the 2 arrays. This makes our coding quite a bit shorter than if we have to write out each subtraction."
      ],
      "metadata": {
        "id": "6B15GjQAN0rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_point is the new data point, with multiple features\n",
        "# row is one row of the DataFrame, which is one existing data point\n",
        "def distance_from_point(new_point, row):\n",
        "  # convert each data point into an array\n",
        "  new_point = np.array(new_point)\n",
        "  row = np.array(row)\n",
        "  # subtract the 2 arrays\n",
        "  # this step subtracts corresponding x, y, z, w values if there are 4 features\n",
        "  difference = new_point - row\n",
        "  # square each difference\n",
        "  squared_difference = difference ** 2\n",
        "  # add the squared differences and take the square root\n",
        "  D = np.sqrt(np.sum(squared_difference))\n",
        "  return D"
      ],
      "metadata": {
        "id": "XYOGi6wjRNq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>1b. Distance between the new data point and all existing data points</u>\n",
        "\n",
        "We now use the `distance_from_point` function above and the `apply` function to find the distance between the new data point and all existing data points or all rows of the DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "4pi1uN1ET0sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_point is the new data point\n",
        "# dataset is the DataFrame of all data\n",
        "def all_distances(new_point, dataset):\n",
        "    # remove the 'Class' column so only the features remain\n",
        "    features = dataset.drop(columns='Class')\n",
        "    # use 'apply' to run 'distance_from_point' with each row of the DataFrame\n",
        "    distances = features.apply(distance_from_point, axis=1, args=(new_point,))\n",
        "                               # args=(new_point,)  is used to send new_point to\n",
        "                               # distance_from_point\n",
        "    return distances"
      ],
      "metadata": {
        "id": "8x_U2FDfUs2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wNa2-zAVJZV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k Nearest Neighbors\n",
        "\n",
        "After we find all the distances, we store them as a column in the DataFrame. Then we sort the DataFrame by the distances, and choose the top k rows, which are the data with the smallest distances."
      ],
      "metadata": {
        "id": "KSjAgV6LWXwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_k_nearest_neighbors(new_point, dataset, k):\n",
        "    # find all distances\n",
        "    distances = all_distances(new_point, dataset)\n",
        "    # create a DataFrame which is the dataset and the distances\n",
        "    results = dataset.join(pd.DataFrame({'Distance':distances}))\n",
        "    # sort the dataset by 'Distance', then take the top k rows\n",
        "    k_nearest_neighbors = results.sort_values(by='Distance').head(k)\n",
        "    return k_nearest_neighbors"
      ],
      "metadata": {
        "id": "b7zsm-vlXAj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "y_FvoCoEJgh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictions\n",
        "\n",
        "From the k nearest neighbors, we write a function to find the class that occurs the most, and return the class as our prediction."
      ],
      "metadata": {
        "id": "ulIg3MxGXzL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def majority(k_nearest):\n",
        "    # find the count of 1's\n",
        "    ones = np.sum(k_nearest.Class == 1)\n",
        "    # find the count of 0's\n",
        "    zeros = np.sum(k_nearest.Class == 0)\n",
        "    # return the larger of the two\n",
        "    if ones > zeros:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# main function of our classifier\n",
        "# accepts as input:\n",
        "#   the DataFrame of existing data,\n",
        "#   the new data point,\n",
        "#   the number of nearest neighbors\n",
        "def classify(new_point, dataset, k):\n",
        "    k_nearest = find_k_nearest_neighbors(new_point, dataset, k)\n",
        "    return majority(k_nearest)"
      ],
      "metadata": {
        "id": "_2OjlWBQYWks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now test the classifier with some known data in the datset.\n",
        "For each test data we remove the `Class` column so the classifier doesn't know the class of the data, then we pass it to the `classify` function to see what the predicted class is."
      ],
      "metadata": {
        "id": "un4TshVUZjTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the 1st row as test data, removing the 'Class' or group data\n",
        "new_wine = training_set.drop(columns='Class').iloc[0]\n",
        "print(\"Predicted group:\", classify(new_wine, training_set, 5))\n",
        "print(\"Actual group:\", training_set.Class.iloc[0])"
      ],
      "metadata": {
        "id": "lvOttESnaGDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the 3rd row as test data\n",
        "new_wine = training_set.drop(columns='Class').iloc[2]\n",
        "print(new_wine.shape)\n",
        "print(\"Predicted group:\", classify(new_wine, training_set, 5))\n",
        "print(\"Actual group:\", training_set.Class.iloc[2])"
      ],
      "metadata": {
        "id": "MJjeDUy5ZbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far so good. The classifier is able to determine correctly the group of the 2 sample test data most of the time.\n",
        "\n",
        "We now test the classifier with our `testing_set` data which we reserved from the original dataset."
      ],
      "metadata": {
        "id": "_7flD9cYa55Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "nc2wsPOFbcK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy\n",
        "\n",
        "First we save the `Class` or group data of `testing_set`, then we drop the `Class` data from the `testing_set`.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ro7q3DebdrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual = testing_set.Class\n",
        "test_set = testing_set.drop(columns='Class')"
      ],
      "metadata": {
        "id": "OgEXWt6dhObQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we use `apply` to run the classifier with `test_set`. The `args` value for `apply` is the input for `classify`, which are the `training_set` and 5, the number of nearest neighbors."
      ],
      "metadata": {
        "id": "1Fiw6wAWhkrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = test_set.apply(classify, axis=1, args=(training_set, 5))"
      ],
      "metadata": {
        "id": "h1Xoa2D-lMsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We store the `actual` group and the `predicted` group in a DataFrame and print the first 5 rows."
      ],
      "metadata": {
        "id": "Wj876QFEsAKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({'Actual': actual, 'Predicted': predicted})\n",
        "results.head(5)"
      ],
      "metadata": {
        "id": "KUhEMeklmduo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that most of the predictions in the first 5 rows are correct.\n",
        "\n",
        "We calculate the percent accuracy by finding the ratio of correct predictions over the total predictions."
      ],
      "metadata": {
        "id": "-rqwfYVGsKnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add all the rows where Actual is the same as Predicted\n",
        "correct = (results.Actual == results.Predicted).sum()\n",
        "# find the ratio of correct predictions over total predictions as a percentage\n",
        "print(\"Accuracy:\", round(correct / len(results) * 100))"
      ],
      "metadata": {
        "id": "ByND_W2vnBl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is not bad for a relatively simple algorithm."
      ],
      "metadata": {
        "id": "dQh9v4fqtBg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "X69UJmxMKca1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now test the classifier with a completely new set of data.\n",
        "\n",
        "Using the textbook dataset, we read in data that can be used for breast cancer detection. The data are measurements of biopsy images."
      ],
      "metadata": {
        "id": "jKebNGUztKAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/breast-cancer.csv\"\n",
        "patients = pd.read_csv(url)\n",
        "rows, columns = patients.shape\n",
        "print(\"Number of rows:\", rows)\n",
        "print(\"Number of columns:\", columns)\n",
        "print(\"First 5 rows:\")\n",
        "patients.head()"
      ],
      "metadata": {
        "id": "f4sMTlDDvlag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ID column is not relevant to the cancer data so we remove it."
      ],
      "metadata": {
        "id": "64z9Ia-vwA2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patients = patients.drop(columns='ID')\n",
        "print(\"First 5 rows:\")\n",
        "patients.head()"
      ],
      "metadata": {
        "id": "xMEdIGy7wIl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data in the columns are in the same range, we don't need to use standard units.\n",
        "\n",
        "We divide the data into the training and testing set."
      ],
      "metadata": {
        "id": "xA790suVwh6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = patients.sample(frac=1)\n",
        "training_set = shuffled_data.iloc[:len(shuffled_data)//2].copy()\n",
        "testing_set = shuffled_data.iloc[len(shuffled_data)//2:].copy()"
      ],
      "metadata": {
        "id": "w4F51N0Qwm5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run `classify` with the 2 datasets."
      ],
      "metadata": {
        "id": "fvj0vF9Vw_QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the actual group, and drop the group from the testing_set\n",
        "actual = testing_set.Class\n",
        "test_set = testing_set.drop(columns='Class')\n",
        "\n",
        "# run the classifier to get the predicted group\n",
        "predicted = test_set.apply(classify, axis=1, args=(training_set, 5))\n",
        "\n",
        "# combine the actual and predicted into a DataFrame, and find the number\n",
        "# of correct predictions\n",
        "correct = (actual == predicted).sum()\n",
        "print(\"Accuracy:\", round(correct / len(actual) * 100))"
      ],
      "metadata": {
        "id": "uTLNeljrxs3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the classifier has high accuracy on a new dataset which is completely different from the wine dataset that we used to develop the algorithm."
      ],
      "metadata": {
        "id": "15ZGufkZ5IZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "WLLo4oas53hL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we use our understanding of how k nearest neighbors work to write an algorithm or a classifier for binary classifcation. Then we test the algorithm using training and testing sets that come from the original dataset. Data in the training set are neighbors to each data point in the testing set. The classifier predicts the group of each data point in the testing set, and by comparing all the predicted groups against the actual groups, we can measure the accuracy of the classifier."
      ],
      "metadata": {
        "id": "cMxptTIn55lJ"
      }
    }
  ]
}